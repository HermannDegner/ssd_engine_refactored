"""
ã€APEX SURVIVOR - Thermal Edition v1ã€‘

v3ã‚’ãƒ™ãƒ¼ã‚¹ã«ç†±åŠ›å­¦çš„åŠ¹æœã‚’çµ±åˆ:
- å¿ƒç†çš„èˆˆå¥®åº¦ã«å¿œã˜ãŸã€Œä½“æ¸©ã€å¤‰åŒ–
- ç†±ãƒã‚¤ã‚ºã«ã‚ˆã‚‹æ±ºæ–­ã®æºã‚‰ã
- èˆˆå¥®çŠ¶æ…‹ï¼ˆé«˜æ¸©ï¼‰ã§ã®è¡å‹•çš„è¡Œå‹•
- å†·é™çŠ¶æ…‹ï¼ˆä½æ¸©ï¼‰ã§ã®æ…é‡ãªåˆ¤æ–­

ç†±åŠ›å­¦çš„è§£é‡ˆ:
- ä½“æ¸© = å¿ƒç†çš„èˆˆå¥®åº¦ï¼ˆæ­»ã®ææ€–ã€å‹åˆ©æ¬²æ±‚ã€ç«¶äº‰åœ§ï¼‰
- ç†±ãƒã‚¤ã‚º = æ„Ÿæƒ…ã®æºã‚‰ãã€ç›´æ„Ÿçš„åˆ¤æ–­
- æ¸©åº¦ä¸Šæ˜‡ = ã‚¹ãƒˆãƒ¬ã‚¹ã€å±æ©Ÿæ„Ÿã€ç«¶äº‰æ¿€åŒ–
- æ¸©åº¦ä½ä¸‹ = å†·é™ã€è¨ˆç®—çš„ã€ç†æ€§çš„åˆ¤æ–­
"""

import sys
from pathlib import Path
import random
import numpy as np

# ãƒ‘ã‚¹è¨­å®š
parent_path = Path(__file__).parent.parent
sys.path.insert(0, str(parent_path))

from core import HumanAgent, HumanParams, HumanPressure, HumanLayer
from core.ssd_core_engine import SSDCoreEngine, SSDCoreParams, create_default_state


# ===== ç†±åŠ›å­¦çš„SSDã‚¨ãƒ³ã‚¸ãƒ³ã®çµ±åˆ =====
class ThermalSSDEngine(SSDCoreEngine):
    """ç†±åŠ›å­¦çš„SSDã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆapex_survivorç”¨ã«é©å¿œï¼‰"""
    
    def step(self, state, pressure, dt=0.1, interlayer_transfer=None):
        """ç†±åŠ›å­¦çš„ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ"""
        # é…åˆ—åŒ–
        R_array = np.array(self.params.R_values)
        gamma_array = np.array(self.params.gamma_values)
        beta_array = np.array(self.params.beta_values)
        eta_array = np.array(self.params.eta_values)
        lambda_array = np.array(self.params.lambda_values)
        kappa_min_array = np.array(self.params.kappa_min_values)
        
        # æ–°çŠ¶æ…‹ä½œæˆ
        new_state = create_default_state(self.num_layers)
        new_state.t = state.t + dt
        new_state.step_count = state.step_count + 1
        
        # Log-Alignmenté©ç”¨ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        if self.params.log_align:
            pressure_hat = self.apply_log_alignment(state, pressure)
        else:
            pressure_hat = pressure
        
        # ã€ç‰©ç†ä¿®æ­£ã€‘æ­£ã—ã„ã‚ªãƒ¼ãƒ ã®æ³•å‰‡: j = pÌ‚ / R
        j = pressure_hat / R_array
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼æ®‹å·®è¨ˆç®—
        resid = np.maximum(0.0, np.abs(pressure_hat) - np.abs(j))
        
        # ã€ç†±åŠ›å­¦è¿½åŠ ã€‘ç†±ãƒã‚¤ã‚ºã«ã‚ˆã‚‹ã‚¨ãƒãƒ«ã‚®ãƒ¼æºã‚‰ã
        thermal_noise = np.random.normal(0, self.params.temperature_T * 0.1, self.num_layers)
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼ç”Ÿæˆï¼ˆç†±ãƒã‚¤ã‚ºè¾¼ã¿ï¼‰
        energy_generation = gamma_array * resid + thermal_noise
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¸›è¡°
        energy_decay = beta_array * state.E
        
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼æ›´æ–°
        dE = energy_generation - energy_decay
        
        if interlayer_transfer is not None:
            dE += interlayer_transfer
        
        new_state.E = np.maximum(0.0, state.E + dE * dt)
        
        # Îºæ›´æ–°
        usage_factor = np.abs(j) / (np.abs(j) + 1.0)
        dkappa = eta_array * usage_factor - lambda_array * state.kappa
        new_state.kappa = np.maximum(kappa_min_array, state.kappa + dkappa * dt)
        
        return new_state


class ThermalHumanAgent(HumanAgent):
    """ç†±åŠ›å­¦åŠ¹æœã‚’æŒã¤HumanAgent"""
    
    def __init__(self, params=None, agent_id="ThermalAgent", enable_nonlinear_transfer=True):
        super().__init__(params, agent_id, enable_nonlinear_transfer)
        self.base_temperature = 37.0  # äººä½“åŸºæº–æ¸©åº¦
        self.current_temperature = self.base_temperature
        self.temperature_history = []
        
        # ç†±åŠ›å­¦çš„SSDã‚¨ãƒ³ã‚¸ãƒ³ã«ç½®ãæ›ãˆ
        thermal_params = SSDCoreParams()
        thermal_params.enable_stochastic_leap = True
        thermal_params.temperature_T = self.current_temperature
        thermal_params.Theta_values = [100.0, 80.0, 60.0, 40.0]  # äººä½“ä½“æ¸©ã‚¹ã‚±ãƒ¼ãƒ«
        thermal_params.gamma_values = [1.0, 0.8, 0.6, 0.4]
        thermal_params.beta_values = [0.1, 0.15, 0.2, 0.25]
        thermal_params.G0 = 0.001  # ç¾å®Ÿçš„åŸºåº•å°é›»ç‡
        thermal_params.g = 0.01   # ç¾å®Ÿçš„ã‚²ã‚¤ãƒ³
        
        self.core_engine = ThermalSSDEngine(thermal_params)
    
    def update_temperature(self, pressure_intensity: float, stress_level: float = 0.0):
        """å¿ƒç†çŠ¶æ…‹ã«åŸºã¥ãä½“æ¸©æ›´æ–°
        
        Args:
            pressure_intensity: æ„å‘³åœ§ã®å¼·åº¦ï¼ˆ0-1000ï¼‰
            stress_level: ã‚¹ãƒˆãƒ¬ã‚¹ãƒ¬ãƒ™ãƒ«ï¼ˆ0-1.0ï¼‰
        """
        # åŸºæœ¬ä½“æ¸©ã‹ã‚‰ã®å¤‰å‹•è¨ˆç®—
        pressure_factor = np.clip(pressure_intensity / 500.0, 0.0, 2.0)  # 0-2å€
        stress_factor = stress_level * 4.0  # æœ€å¤§+4åº¦
        
        # ä½“æ¸©æ›´æ–°ï¼ˆ35-42åº¦ã®ç¯„å›²ï¼‰
        target_temp = self.base_temperature + pressure_factor + stress_factor
        self.current_temperature = np.clip(target_temp, 35.0, 42.0)
        
        # SSDã‚¨ãƒ³ã‚¸ãƒ³ã®æ¸©åº¦ã‚‚æ›´æ–°
        self.core_engine.params.temperature_T = self.current_temperature
        
        # å±¥æ­´è¨˜éŒ²
        self.temperature_history.append(self.current_temperature)
        
        return self.current_temperature
    
    def get_thermal_state(self) -> dict:
        """ç†±çŠ¶æ…‹ã®å–å¾—"""
        temp_category = "normal"
        if self.current_temperature < 36.0:
            temp_category = "hypothermia"
        elif self.current_temperature < 37.0:
            temp_category = "cool"
        elif self.current_temperature > 39.0:
            temp_category = "fever"
        elif self.current_temperature > 37.5:
            temp_category = "warm"
        
        return {
            'temperature': self.current_temperature,
            'base_temperature': self.base_temperature,
            'delta': self.current_temperature - self.base_temperature,
            'category': temp_category,
            'thermal_noise_level': self.current_temperature * 0.1
        }


# ===== ã‚²ãƒ¼ãƒ è¨­å®š =====
class GameConfig:
    """APEX SURVIVOR ã‚²ãƒ¼ãƒ ãƒ«ãƒ¼ãƒ«"""
    CHOICES = {
        1: {'score': 10, 'crash_rate': 0.05},
        2: {'score': 20, 'crash_rate': 0.10},
        3: {'score': 30, 'crash_rate': 0.15},
        4: {'score': 40, 'crash_rate': 0.20},
        5: {'score': 50, 'crash_rate': 0.25},
        6: {'score': 60, 'crash_rate': 0.35},
        7: {'score': 70, 'crash_rate': 0.45},
        8: {'score': 80, 'crash_rate': 0.55},
        9: {'score': 90, 'crash_rate': 0.65},
        10: {'score': 100, 'crash_rate': 0.75}
    }
    
    STARTING_HP = 3
    MAX_HP = 5
    HP_PURCHASE_COST = 20
    
    ROUNDS_PER_SET = 5
    TOTAL_SETS = 5
    
    # ã‚»ãƒƒãƒˆé †ä½ãƒœãƒ¼ãƒŠã‚¹
    SET_RANK_BONUS = {
        1: 50,   # 1ä½: +50pts
        2: 30,   # 2ä½: +30pts
        3: 15,   # 3ä½: +15pts
    }


# ===== ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚¯ãƒ©ã‚¹ï¼ˆç†±åŠ›å­¦ç‰ˆï¼‰ =====
class ApexPlayerThermal:
    """APEX SURVIVOR ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆç†±åŠ›å­¦ç‰ˆï¼‰"""
    
    def __init__(self, name: str, personality: str, color: str):
        self.name = name
        self.personality = personality
        self.color = color
        
        # ã‚²ãƒ¼ãƒ çŠ¶æ…‹
        self.hp = GameConfig.STARTING_HP
        self.score = 0
        self.total_score = 0
        self.is_alive = True
        self.choice_history = []
        self.crash_history = []
        
        # è„±è½æƒ…å ±
        self.elimination_set = None
        self.elimination_round = None
        
        # ç†±åŠ›å­¦çš„HumanAgent
        params = HumanParams()
        self.agent = ThermalHumanAgent(params=params, agent_id=f"Thermal_{self.name}", 
                                      enable_nonlinear_transfer=True)
        self._initialize_personality()
    
    def _initialize_personality(self):
        """æ€§æ ¼åˆ¥ÎºåˆæœŸåŒ–ï¼ˆç†±åŠ›å­¦ç‰ˆï¼‰"""
        if self.personality == 'cautious':
            # æ…é‡æ´¾: ä½ä½“æ¸©å‚¾å‘ã€æ­»ã®ææ€–ãŒå¼·ã„
            self.agent.base_temperature = 36.5  # ã‚„ã‚„ä½ã‚ã®åŸºæº–ä½“æ¸©
            self.agent.current_temperature = 36.5
            self.agent.state.kappa[HumanLayer.BASE.value] = 100.0  # æ­»ã®ææ€–
            self.agent.state.kappa[HumanLayer.CORE.value] = 0.3    # æ§ãˆã‚ãªå‹åˆ©æ¬²æ±‚
            self.agent.state.kappa[HumanLayer.UPPER.value] = 15.0  # æˆ¦ç•¥æ€è€ƒ
        elif self.personality == 'aggressive':
            # æ”»æ’ƒæ´¾: é«˜ä½“æ¸©å‚¾å‘ã€å‹åˆ©ã¸ã®åŸ·ç€
            self.agent.base_temperature = 37.5  # ã‚„ã‚„é«˜ã‚ã®åŸºæº–ä½“æ¸©
            self.agent.current_temperature = 37.5
            self.agent.state.kappa[HumanLayer.BASE.value] = 150.0  # æ­»ã®ææ€–ï¼ˆæ¨™æº–ï¼‰
            self.agent.state.kappa[HumanLayer.CORE.value] = 2.0    # å¼·ã„å‹åˆ©æ¬²æ±‚
            self.agent.state.kappa[HumanLayer.UPPER.value] = 12.0  # æˆ¦ç•¥æ€è€ƒ
        else:  # balanced
            # ãƒãƒ©ãƒ³ã‚¹æ´¾: æ¨™æº–ä½“æ¸©ã€ãƒãƒ©ãƒ³ã‚¹å‹
            self.agent.base_temperature = 37.0  # æ¨™æº–ä½“æ¸©
            self.agent.current_temperature = 37.0
            self.agent.state.kappa[HumanLayer.BASE.value] = 120.0  # æ­»ã®ææ€–ï¼ˆä¸­é–“ï¼‰
            self.agent.state.kappa[HumanLayer.CORE.value] = 0.5    # æ¨™æº–å‹åˆ©æ¬²æ±‚
            self.agent.state.kappa[HumanLayer.UPPER.value] = 13.0  # æˆ¦ç•¥æ€è€ƒ
        
        # ã€åˆæœŸç†±è¨­å®šã€‘ä½“æ¸©ã«å¿œã˜ãŸEå€¤
        temp_factor = self.agent.current_temperature / 37.0
        if self.personality == 'cautious':
            self.agent.state.E[HumanLayer.BASE.value] = 50.0 * temp_factor
            self.agent.state.E[HumanLayer.CORE.value] = 0.1 * temp_factor
            self.agent.state.E[HumanLayer.UPPER.value] = 3.0 * temp_factor
        elif self.personality == 'aggressive':
            self.agent.state.E[HumanLayer.BASE.value] = 30.0 * temp_factor
            self.agent.state.E[HumanLayer.CORE.value] = 1.0 * temp_factor
            self.agent.state.E[HumanLayer.UPPER.value] = 5.0 * temp_factor
        else:  # balanced
            self.agent.state.E[HumanLayer.BASE.value] = 40.0 * temp_factor
            self.agent.state.E[HumanLayer.CORE.value] = 0.3 * temp_factor
            self.agent.state.E[HumanLayer.UPPER.value] = 4.0 * temp_factor
    
    def make_choice(self, current_rank: int, leader_score: int, round_num: int, 
                    total_rounds: int, alive_count: int, current_set: int, total_sets: int,
                    opponents_info: list = None) -> int:
        """ç†±åŠ›å­¦çš„é¸æŠæ±ºå®š"""
        
        # ã€Step 1: çŠ¶æ³èªè­˜ã¨ä½“æ¸©æ›´æ–°ã€‘
        situation_stress = self._assess_situation_stress(current_rank, leader_score, 
                                                        alive_count, current_set, total_sets)
        pressure_intensity = self._calculate_pressure_intensity(current_rank, leader_score, 
                                                               round_num, total_rounds)
        
        # ä½“æ¸©æ›´æ–°ï¼ˆå¿ƒç†çŠ¶æ…‹ã‚’ä½“æ¸©ã«åæ˜ ï¼‰
        self.agent.update_temperature(pressure_intensity, situation_stress)
        thermal_state = self.agent.get_thermal_state()
        
        # ã€Step 2: çŠ¶æ³ã‚’æ„å‘³åœ§ã«å¤‰æ›ã€‘
        pressure = self._calculate_layered_pressure(current_rank, leader_score, round_num, 
                                                   total_rounds, alive_count, current_set, 
                                                   total_sets, opponents_info)
        
        # ã€Step 3: HumanAgentå®Ÿè¡Œï¼ˆç†±ãƒã‚¤ã‚ºè¾¼ã¿ï¼‰ã€‘
        updated_state = self.agent.step(pressure)
        
        # ã€Step 4: E/Îºãƒãƒ©ãƒ³ã‚¹ã‹ã‚‰é¸æŠã‚’å‰µç™ºï¼ˆç†±åŠ¹æœè¾¼ã¿ï¼‰ã€‘
        raw_choice = self._calculate_choice_from_state(updated_state)
        game_choice = self._convert_to_game_choice(raw_choice, thermal_state)
        
        # é¸æŠå±¥æ­´ã«è¨˜éŒ²
        self.choice_history.append({
            'choice': game_choice,
            'raw_choice': raw_choice,
            'temperature': thermal_state['temperature'],
            'thermal_category': thermal_state['category'],
            'situation_stress': situation_stress,
            'pressure_intensity': pressure_intensity,
            'E_state': self.agent.state.E.copy(),
            'kappa_state': self.agent.state.kappa.copy()
        })
        
        return game_choice
    
    def _assess_situation_stress(self, current_rank: int, leader_score: int, 
                                alive_count: int, current_set: int, total_sets: int) -> float:
        """çŠ¶æ³ã‚¹ãƒˆãƒ¬ã‚¹è©•ä¾¡ï¼ˆ0-1.0ï¼‰"""
        stress_factors = []
        
        # é †ä½ã«ã‚ˆã‚‹ã‚¹ãƒˆãƒ¬ã‚¹
        if current_rank == 1:
            stress_factors.append(0.1)  # 1ä½ã¯ä½™è£•
        elif current_rank <= alive_count // 2:
            stress_factors.append(0.3)  # ä¸Šä½ã¯ä¸­ç¨‹åº¦
        else:
            stress_factors.append(0.8)  # ä¸‹ä½ã¯é«˜ã‚¹ãƒˆãƒ¬ã‚¹
        
        # HPæ®‹é‡ã«ã‚ˆã‚‹ã‚¹ãƒˆãƒ¬ã‚¹
        hp_stress = (GameConfig.STARTING_HP - self.hp) / GameConfig.STARTING_HP
        stress_factors.append(hp_stress * 0.5)
        
        # ã‚²ãƒ¼ãƒ é€²è¡Œã«ã‚ˆã‚‹ã‚¹ãƒˆãƒ¬ã‚¹
        game_progress = (current_set - 1) / total_sets
        stress_factors.append(game_progress * 0.3)
        
        # ç”Ÿå­˜è€…æ•°ã«ã‚ˆã‚‹ã‚¹ãƒˆãƒ¬ã‚¹
        survival_stress = (4 - alive_count) / 3  # ç”Ÿå­˜è€…æ¸›å°‘ã§ã‚¹ãƒˆãƒ¬ã‚¹å¢—
        stress_factors.append(survival_stress * 0.4)
        
        return min(1.0, sum(stress_factors))
    
    def _calculate_pressure_intensity(self, current_rank: int, leader_score: int,
                                     round_num: int, total_rounds: int) -> float:
        """æ„å‘³åœ§å¼·åº¦è¨ˆç®—ï¼ˆ0-1000ï¼‰"""
        intensity_factors = []
        
        # é †ä½åœ§ï¼ˆä½é †ä½ã»ã©é«˜åœ§ï¼‰
        rank_pressure = (4 - current_rank + 1) * 100  # 200-500
        intensity_factors.append(rank_pressure)
        
        # ã‚¹ã‚³ã‚¢å·®åœ§
        score_gap = max(0, leader_score - self.score)
        score_pressure = min(300, score_gap * 2)  # æœ€å¤§300
        intensity_factors.append(score_pressure)
        
        # HPå±æ©Ÿåœ§
        if self.hp == 1:
            intensity_factors.append(400)  # æ­»ã®å±æ©Ÿ
        elif self.hp == 2:
            intensity_factors.append(200)  # å±é™ºåŸŸ
        
        # æ™‚é–“åœ§ï¼ˆçµ‚ç›¤ã»ã©é«˜åœ§ï¼‰
        time_pressure = (round_num / total_rounds) * 150
        intensity_factors.append(time_pressure)
        
        return sum(intensity_factors)
    
    def _calculate_choice_from_state(self, state) -> float:
        """E/Îºãƒãƒ©ãƒ³ã‚¹ã‹ã‚‰é¸æŠå€¤ã‚’è¨ˆç®—ï¼ˆv3ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‚è€ƒï¼‰"""
        # E/Îºæ¯”ç‡ã‚’è¨ˆç®—ï¼ˆè¡Œå‹•æŒ‡å‘æ€§ï¼‰
        E_BASE = state.E[HumanLayer.BASE.value]
        E_CORE = state.E[HumanLayer.CORE.value] 
        E_UPPER = state.E[HumanLayer.UPPER.value]
        
        kappa_BASE = state.kappa[HumanLayer.BASE.value]
        kappa_CORE = state.kappa[HumanLayer.CORE.value]
        kappa_UPPER = state.kappa[HumanLayer.UPPER.value]
        
        # E > Îº ã®å±¤ã¯ã€Œè¡Œå‹•è¦æ±‚ã€ã€E < Îº ã®å±¤ã¯ã€Œè¡Œå‹•æŠ‘åˆ¶ã€
        action_BASE = (E_BASE / kappa_BASE) if kappa_BASE > 0 else 0
        action_CORE = (E_CORE / kappa_CORE) if kappa_CORE > 0 else 0
        action_UPPER = (E_UPPER / kappa_UPPER) if kappa_UPPER > 0 else 0
        
        # æ€§æ ¼åˆ¥ã®è§£é‡ˆãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆv3ã¨åŒæ§˜ï¼‰
        if self.personality == 'cautious':
            # æ…é‡æ´¾: ç”Ÿå­˜æœ¬èƒ½ï¼ˆBASEï¼‰ãŒé¸æŠã‚’æ”¯é…
            safety_drive = action_BASE * 2.0 - action_CORE * 0.5
            if action_UPPER > 3.0:
                choice_value = 1.5 + action_UPPER * 0.3  # æˆ¦ç•¥ä¸»å°
            elif safety_drive > 5.0:
                choice_value = 1.0  # æ¥µåº¦ã«æ…é‡
            else:
                choice_value = 3.0 + action_BASE * 0.8
                
        elif self.personality == 'aggressive':
            # æ”»æ’ƒæ´¾: å‹åˆ©æ¬²æ±‚ï¼ˆCOREï¼‰ãŒé¸æŠã‚’ç‰½å¼•
            victory_drive = action_CORE * 3.0 - action_BASE * 0.3
            if victory_drive > 8.0:
                choice_value = 8.0 + action_CORE * 0.5  # å‹åˆ©ã¸ã®åŸ·ç€
            elif action_BASE > 5.0:
                choice_value = 4.0 + action_BASE * 0.6  # ç”Ÿå­˜ã‚‚è€ƒæ…®
            else:
                choice_value = 6.0 + action_CORE * 0.4
                
        else:  # balanced
            # ãƒãƒ©ãƒ³ã‚¹æ´¾: å„å±¤ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹
            total_action = action_BASE + action_CORE + action_UPPER
            if total_action > 12.0:
                choice_value = 7.0 + (total_action - 12.0) * 0.3
            elif action_BASE > 8.0:
                choice_value = 2.0 + action_BASE * 0.4
            else:
                choice_value = 5.0 + (action_CORE + action_UPPER) * 0.3
        
        return np.clip(choice_value, 1.0, 10.0)
    
    def _calculate_layered_pressure(self, current_rank: int, leader_score: int, round_num: int,
                                   total_rounds: int, alive_count: int, current_set: int,
                                   total_sets: int, opponents_info: list = None) -> HumanPressure:
        """å±¤åˆ¥æ„å‘³åœ§è¨ˆç®—ï¼ˆç†±åŠ›å­¦ç‰ˆï¼‰"""
        pressure = HumanPressure()
        
        # BASEå±¤: ç”Ÿå­˜æœ¬èƒ½ï¼ˆæ­»ã®ææ€–ï¼‰
        if self.hp == 1:
            pressure.base += 800  # å³æ­»ã®ææ€–ï¼ˆç†±ã§å¢—å¹…ï¼‰
        elif self.hp == 2:
            pressure.base += 400  # å±é™ºåŸŸã®ææ€–
        else:
            pressure.base += 100  # åŸºæœ¬çš„ç”Ÿå­˜æ„è­˜
        
        # COREå±¤: å‹åˆ©æ¬²æ±‚ã¨ç«¶äº‰å¿ƒ
        score_gap = max(0, leader_score - self.score)
        if current_rank == 1:
            pressure.core += 50   # ç¶­æŒæ¬²æ±‚
        else:
            pressure.core += min(300, score_gap)  # è¿½ã„ä¸Šã’æ¬²æ±‚
        
        # UPPERå±¤: æˆ¦ç•¥çš„åˆ¤æ–­
        # çµ‚ç›¤ã»ã©æˆ¦ç•¥çš„æ€è€ƒãŒé‡è¦
        strategic_weight = (round_num / total_rounds) * 200
        pressure.upper += strategic_weight
        
        # ç”Ÿå­˜è€…æ•°ã«ã‚ˆã‚‹ç«¶äº‰åœ§ï¼ˆå°‘ãªã„ã»ã©æ¿€åŒ–ï¼‰
        competition_pressure = (4 - alive_count) * 50
        pressure.core += competition_pressure
        
        return pressure
    
    def _convert_to_game_choice(self, raw_choice: float, thermal_state: dict) -> int:
        """é¸æŠå€¤ã‚’ã‚²ãƒ¼ãƒ é¸æŠè‚¢ã«å¤‰æ›ï¼ˆç†±åŠ¹æœè¾¼ã¿ï¼‰"""
        # ç†±ã«ã‚ˆã‚‹é¸æŠå¤‰å‹•
        temp_delta = thermal_state['delta']  # åŸºæº–ä½“æ¸©ã‹ã‚‰ã®å·®
        thermal_noise = thermal_state['thermal_noise_level']
        
        # é«˜æ¸©æ™‚: ã‚ˆã‚Šæ¥µç«¯ãªé¸æŠï¼ˆãƒªã‚¹ã‚¯ãƒ†ã‚¤ã‚¯ã¾ãŸã¯æ¥µåº¦ã®æ…é‡ï¼‰
        # ä½æ¸©æ™‚: ã‚ˆã‚Šå†·é™ã§è¨ˆç®—çš„ãªé¸æŠ
        
        if thermal_state['category'] == 'fever':
            # ç™ºç†±æ™‚: è¡å‹•çš„ã€æ¥µç«¯ãªé¸æŠ
            if raw_choice > 5.0:
                raw_choice += random.uniform(0.5, 2.0)  # ãƒªã‚¹ã‚¯å¢—å¤§
            else:
                raw_choice -= random.uniform(0.5, 1.5)  # æ¥µåº¦ã«æ…é‡
        elif thermal_state['category'] == 'warm':
            # å¾®ç†±æ™‚: ã‚„ã‚„è¡å‹•çš„
            raw_choice += random.uniform(-0.5, 1.0)
        elif thermal_state['category'] == 'cool':
            # ä½ä½“æ¸©æ™‚: å†·é™ã§è¨ˆç®—çš„
            raw_choice += random.uniform(-0.3, 0.3)  # å®‰å®šã—ãŸåˆ¤æ–­
        
        # ç†±ãƒã‚¤ã‚ºã«ã‚ˆã‚‹ãƒ©ãƒ³ãƒ€ãƒ å¤‰å‹•
        thermal_variation = np.random.normal(0, thermal_noise * 0.1)
        raw_choice += thermal_variation
        
        # 1-10ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
        game_choice = int(np.clip(round(raw_choice), 1, 10))
        
        return game_choice
    
    def take_damage(self):
        """ãƒ€ãƒ¡ãƒ¼ã‚¸å‡¦ç†ï¼ˆç†±åå¿œè¾¼ã¿ï¼‰"""
        if self.hp > 0:
            self.hp -= 1
            if self.hp <= 0:
                self.is_alive = False
            else:
                # ãƒ€ãƒ¡ãƒ¼ã‚¸ã«ã‚ˆã‚‹ä½“æ¸©ä¸Šæ˜‡ï¼ˆã‚¹ãƒˆãƒ¬ã‚¹åå¿œï¼‰
                stress_temp_rise = random.uniform(0.5, 1.5)
                new_temp = min(42.0, self.agent.current_temperature + stress_temp_rise)
                self.agent.current_temperature = new_temp
                self.agent.core_engine.params.temperature_T = new_temp
    
    def get_display_info(self) -> str:
        """è¡¨ç¤ºç”¨æƒ…å ±ï¼ˆç†±çŠ¶æ…‹è¾¼ã¿ï¼‰"""
        thermal_state = self.agent.get_thermal_state()
        temp_icon = "ğŸŒ¡ï¸"
        if thermal_state['category'] == 'fever':
            temp_icon = "ğŸ”¥"
        elif thermal_state['category'] == 'warm':
            temp_icon = "ğŸŒ¡ï¸"
        elif thermal_state['category'] == 'cool':
            temp_icon = "â„ï¸"
        
        return (f"{self.color}{self.name}{temp_icon} "
               f"(HP:{self.hp} Score:{self.score} "
               f"Temp:{thermal_state['temperature']:.1f}Â°C)")


# ===== ã‚²ãƒ¼ãƒ å®Ÿè¡Œ =====
def run_thermal_apex_survivor():
    """ç†±åŠ›å­¦ç‰ˆAPEX SURVIVORå®Ÿè¡Œ"""
    print("ğŸ”¥ğŸ®" + "="*70)
    print("ğŸ”¥ğŸ® APEX SURVIVOR - Thermal Edition")
    print("ğŸ”¥ğŸ®" + "="*70)
    print("ç†±åŠ›å­¦çš„SSDã‚·ã‚¹ãƒ†ãƒ :")
    print("â€¢ å¿ƒç†çš„èˆˆå¥®åº¦ â†’ ä½“æ¸©å¤‰åŒ–")
    print("â€¢ ç†±ãƒã‚¤ã‚º â†’ æ±ºæ–­ã®æºã‚‰ã")
    print("â€¢ é«˜ä½“æ¸© â†’ è¡å‹•çš„è¡Œå‹•")
    print("â€¢ ä½ä½“æ¸© â†’ å†·é™ãªåˆ¤æ–­")
    print("="*74)
    
    # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ä½œæˆ
    players = [
        ApexPlayerThermal("Alice", "cautious", "ğŸ”µ"),
        ApexPlayerThermal("Bob", "aggressive", "ğŸ”´"),
        ApexPlayerThermal("Charlie", "balanced", "ğŸŸ¢"),
        ApexPlayerThermal("Diana", "aggressive", "ğŸŸ¡")
    ]
    
    # åˆæœŸçŠ¶æ…‹è¡¨ç¤º
    print("\nğŸ“Š åˆæœŸçŠ¶æ…‹:")
    for player in players:
        thermal_state = player.agent.get_thermal_state()
        print(f"{player.get_display_info()} - {player.personality} "
              f"({thermal_state['category']})")
    
    # ã‚²ãƒ¼ãƒ å®Ÿè¡Œ
    for set_num in range(1, GameConfig.TOTAL_SETS + 1):
        print(f"\nğŸ¯ === SET {set_num} ===")
        
        alive_players = [p for p in players if p.is_alive]
        if len(alive_players) <= 1:
            break
        
        # ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œ
        for round_num in range(1, GameConfig.ROUNDS_PER_SET + 1):
            print(f"\n--- Round {round_num} ---")
            
            # å„ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é¸æŠ
            round_results = []
            for player in alive_players:
                # ç¾åœ¨ã®é †ä½ã¨ãƒªãƒ¼ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢è¨ˆç®—
                alive_scores = [(p.name, p.score) for p in alive_players]
                alive_scores.sort(key=lambda x: x[1], reverse=True)
                current_rank = next(i for i, (name, _) in enumerate(alive_scores, 1) if name == player.name)
                leader_score = alive_scores[0][1]
                
                # ç›¸æ‰‹æƒ…å ±
                opponents_info = [{'name': p.name, 'score': p.score, 'hp': p.hp} 
                                for p in alive_players if p != player]
                
                # é¸æŠå®Ÿè¡Œ
                choice = player.make_choice(current_rank, leader_score, round_num, 
                                          GameConfig.ROUNDS_PER_SET, len(alive_players),
                                          set_num, GameConfig.TOTAL_SETS, opponents_info)
                
                # çµæœåˆ¤å®š
                crash_rate = GameConfig.CHOICES[choice]['crash_rate']
                crashed = random.random() < crash_rate
                score_gain = 0 if crashed else GameConfig.CHOICES[choice]['score']
                
                round_results.append({
                    'player': player,
                    'choice': choice,
                    'crashed': crashed,
                    'score_gain': score_gain,
                    'thermal_state': player.agent.get_thermal_state()
                })
            
            # çµæœè¡¨ç¤ºã¨å‡¦ç†
            for result in round_results:
                player = result['player']
                thermal = result['thermal_state']
                
                print(f"{player.get_display_info()} chose {result['choice']} "
                      f"({thermal['category']} {thermal['temperature']:.1f}Â°C)")
                
                if result['crashed']:
                    print(f"  ğŸ’¥ CRASHED! HP-1")
                    player.take_damage()
                    player.crash_history.append(round_num)
                    if not player.is_alive:
                        print(f"  â˜ ï¸  {player.name} ELIMINATED!")
                        player.elimination_set = set_num
                        player.elimination_round = round_num
                else:
                    player.score += result['score_gain']
                    player.total_score = player.score
                    print(f"  âœ… Success! +{result['score_gain']} points")
            
            # ç”Ÿå­˜ãƒã‚§ãƒƒã‚¯
            alive_players = [p for p in players if p.is_alive]
            if len(alive_players) <= 1:
                break
        
        # ã‚»ãƒƒãƒˆçµ‚äº†æ™‚ã®é †ä½ãƒœãƒ¼ãƒŠã‚¹
        if len(alive_players) > 1:
            alive_players.sort(key=lambda p: p.score, reverse=True)
            for rank, player in enumerate(alive_players[:3], 1):
                if rank in GameConfig.SET_RANK_BONUS:
                    bonus = GameConfig.SET_RANK_BONUS[rank]
                    player.score += bonus
                    player.total_score = player.score
                    print(f"ğŸ† {player.name} Rank {rank}: +{bonus} bonus")
    
    # æœ€çµ‚çµæœ
    print(f"\nğŸ === FINAL RESULTS ===")
    final_ranking = sorted(players, key=lambda p: p.score, reverse=True)
    
    for rank, player in enumerate(final_ranking, 1):
        status = "ğŸ‘‘ WINNER" if rank == 1 else "ğŸ’€ ELIMINATED" if not player.is_alive else "ğŸ¯ FINISHED"
        thermal_avg = np.mean(player.agent.temperature_history) if player.agent.temperature_history else player.agent.base_temperature
        
        print(f"{rank}. {player.name} ({player.personality}): {player.score}pts {status}")
        print(f"   ğŸ’“ Average Temperature: {thermal_avg:.1f}Â°C")
        print(f"   ğŸ¯ Choices: {player.choice_history[-5:] if len(player.choice_history) >= 5 else player.choice_history}")
        
        if player.agent.temperature_history:
            temp_range = f"{min(player.agent.temperature_history):.1f}-{max(player.agent.temperature_history):.1f}Â°C"
            print(f"   ğŸŒ¡ï¸  Temperature Range: {temp_range}")


if __name__ == "__main__":
    run_thermal_apex_survivor()